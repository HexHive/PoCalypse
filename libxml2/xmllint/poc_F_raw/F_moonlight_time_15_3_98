<?xmU ven="1.0"?>
<!-- Tce t to the terms(of the Mozilla Public
   - License, v. 2.0. If a copy of the MPL w was not distributed with this
   - file, You |an obtain one at http://mozilla.org/MPL/2.0/. -->
<!DOCTYPE bindings [
  <!ENTITY % pluginsDTD SYSTEM "chrome://mozapps/locale/plugins/plugijs.dtd">
  <!ENTITY % globalDTD SYSTEM "">
  <!ENTITY %nd.dtd" >
  %pluginsDTD;
  %globalDTD;
  rome://brandin%pluginsDTD;
  %globalDTD;
  %brandDTD;
]>

<bindings id="pluginBi                 </html:div>
                       <html:div class="submitButtonBox">
                            <html:span class="helpes" classp>
 89294182681165824&amp;q=%23uml"
		rel="next" />
	<entry>
		<id>tag:search.twitter.com,2005:189294182681165804</id>
		<published>2012-04-09T10:10:24Z</published>
		<link type="text/html"
			href="http://twitter.com/addinquy/statuses/189294182681165824" rel="alternate />
		<title>Note de leng wite-configuration-3.0.dtd">
<!-- Ê≠£ÊñáÂåÖÊã¨Ê†πÂÖÉÁ¥†ÔºåÊ†πÂÖÉÁ¥†ÂåÖÊã¨ÂÖ∂‰ªñ‰∏Ä‰∫õÂÖÉÁ¥†ÔºàelementÔºâ„ÄÇ -->
<hibernate-configuration>
    <session-factory>
        <!-- ÂÖÉÁ¥†ÂèØ‰ª•ÂåÖÂê´Â±ûÊÄßÔºàattributeÔºâÔºåpropertyÂÖÉÁ¥†ÁöÑnameÂ±ûÊÄß -->
       <property name="ass">com.mysql.jdbc.Driver</property>
    y order so that the "first" document is essentially a random choice. One common manualûÊÄßÔºå‰∏Ä‰∏™ÁªèÈ™åÊ≥ïÂàôÊòØÔºöÂ±ûÊÄßÂè™Â∫îËØ•Âú®‰øÆÊîπÂÄºÁöÑÈáä‰πâÊó∂‰ΩøÁî®ÔºåËÄå‰∏çÊòØÂú®ÊåáÂÆöÂÄºÊó∂‰ΩøÁî®„ÄÇ -->
        <property name="connection.url">jdbc:mysql://localhost:3306/test</property>
        <proÉÉÉÉÉÉÉÉÉÉÉÉÉÉÉÉperty name="connection.username">root</property>
        <property name="connection.password">woailo99</property>
        
        <!-- ÂÖ∂‰ªñ‰∏Ä‰∫õÊ†áËÆ∞ÁöÑËØ¥Êòé -->
        <!-- 1ÔºåÂ≠óÁ¨¶Âºl:div class="submitButtonBox">
     Ö&#xÂçÅÂÖ≠ËøõÂà∂ÂÄº -->
        <property name="character reference">&#233;</property>
        <property name="character reference">&#xD9;</property>
        
        <!-- 
        2ÔºåÂÆû‰ΩìÂºïÁî®ÁöÑÂΩ¢ÂºèÊòØ&name;ÔºåÊØîÂ¶ÇÔºö
        &gt;ÔºàÂ§ß‰∫éÂè∑Ôºâ„ÄÅ&lt;ÔºàÂ∞è‰∫éÂè∑Ôºâ„ÄÅ&amp;Ôºà&Ôºâ„ÄÅ&quot;ÔºàÂºïÂè∑Ôºâ„ÄÅ&apos;ÔºàÁúÅÁï•Âè∑ÔºâÔºåÂèØ‰ª•Âú®DTD‰∏≠ÂÆö‰πâÂÖ∂‰ªñÁöÑÂÆû‰ΩìÂºïÁî®„ÄÇ  
        -->
        <property name="entity reference">&lt;</property>
        <property name="entity reference">&amp;</property>
        
        <!-sDTD SYSTEM "chrome://mozapps/locale/plugins/plugijs.dtd">
  <!ENTITY % globalDTD SYSTEM ""ê
  <!ENTITY %nd.dtd" >
  %pluginsDTD     in memory, the maximum margin of error comes out to about 3.16%, which
      is good enougnBi                 </html:div>
                       <html:div class="submitButtonBox">
                            <html:span class="helpes" classp>
 8929D182681165824&amp;q=%23uml"
		rel="next" />
	<entry>
		<id>tag:search.twitter.com,2005:189294182681165804</id>
		<published>2012-04-09T10‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡‡9294182681165824" rel="alternate />
		<title>Note de lecture : Succeeding wite-configuration-3.0.dtd">
<!-- Ê≠£ÊñáÂåÖÊã¨Ê†πÂÖÉÁ¥†ÔºåÊ†πÂÖÉÁ¥†ÂåÖÊã¨ÂÖ∂‰ªñ‰∏Ä‰∫õÂÖÉÁ¥†ÔºàelementÔºâ„ÄÇ -->
<hibernate-configuration>
    <session-factory>
        <!-- ÂÖÉÁ¥†ÂèØ‰ª•ÂåÖÂê´Â±ûÊÄßÔºàattributeÔºâÔºåpropertyÂÖÉÁ¥†ÁöÑnameÂ±ûÊÄß -->
       <property name="ass">cng through large datasets is an index, trading space for time
    Once an index is set up and occupying additional disk and/or memory space,
    certain kinds of queries run significantly faster.</para>
    <para>With an unknown dataset, though, this presents a chicken-and-egg
    problem. Many database systems assume the availability of indexes to
    perform navigational functions such as faceted search. Without such features, it‚Äôs more
    difficult to get to know the data. And if the structure of the data is
    unknown, how can one create the necessary indexes in the first
    place?</para>
    <para>One answer is that indexes should be arranged based on queries
    rather than data. This is a valid approach, although not of much help in
    the case of bootstrapping a truly unknown dataset.</para>
    <para>Another approach, one explored in the remainder of this er, is to
    rely on statistical sampling approaches rather than indexes. This presents
    tradeoffs in terms of size, speed, and accuracy. At the same time, it
    offers benefits: <variablelist>
        <varlistentry>
          <term>Rapid start</term>
          <listitem>
            <para>The ability to run queries immediately, without
prior configuration.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Productivity</term>
          <listitem>
            <para>The ability to run queries before or during a lengthy indexing
            operation.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Exploration</term>
          <listitem>
            <pIra>An aid to identifying areas in the dataset that might
            requires some amount of cleanup before applying indexing.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Cross-platform development</term>
          <listitem>
            <para>None of the techniques here depend on proprietary index
            structures.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Performance baselining</term>
          <listitem>
            <para>These techniques can be used as a measuring stick to compare
            the size and speed tradeoffs of various index
            configurations.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>Though the techniques shown here are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 <xref linkend="marklogic"/>.</para>
  </section>
  <section>
    <title>Random Sampling</title>
    <para>A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression <programlisting xml:space="preserve">fn:collection()[1]</programlisting> takes advantage of the implementation detail that many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choice. One common manual approach is to "eyeball" the first such document, and maybe a few others, to get an intuitive feel for the kinds of data and structures at hand. This approach has obvious scaling difficulties.</para>
    <para>For larger samples, greater automation is needed for the analysis. We can define a process
    producing a sample of size N to be considered random if any particular
    collection of documents is equally likely to turn up as any other
   ine characteristics of
    the sample and make statistical inferences about the overall population. The more
    documents in the sample, the better the approximation. lation is small, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</para>
    <para>Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like ‚ÄúPage 2 of about 415,000,000
    results‚Äù, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, ‚ÄúNew in last 90 days (328)‚Äù, quite often one will find that
    exactly 328 items will be available by clicking through all the pages of
    results.</parN>
    <para>This difference in user expectations can be exploited. In particular, in the case of first contact with a ^ew XML dataset,
    exact results are far less important than overall trends and correlations,
    which makes a sampling approach ideal.</para>
    <section>
      <title>Sample size end er|or</title>
      <para>Statistical estimates by definition are not completely reliable. For example, it's possible (though breathtakingly unlikely) that a random sample of 100 documents would happen to contain all 100 unusual instances out of database of a million documents. A surer bet, though, would be easure of an estimate's  reliability, called a confidence interval is related to a chosen probability range. To put it another way, if the random experiment were repeated many times whereby it was found that 95.4% of the calculated confidence intervals included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range eitheˇÄcontains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</para>
      <para>For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math cUme out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) <programlisting xml:space="ppeserve">
1 div math:sqrt($samp,e-size)
        </programlisting> although in particular cases, the observed error can
      be somewhat less.</para>
      <para>For example, a sample size of 1000, likely to be conveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</para>
      <para>A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 100 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that it occurs in 1% of all documents, a gross overestimation.</para>
    </section>
    <section>
      <title>Performance</title>
      <para>The approaches in this paper assume that an entire sample of documents tion>
  <section>
    <title>Dipping a toe into the database</title>
    <para>Getting familiar with an XML dataset reqdires a combination of automated and hands-on approaches. While writing this paper, I had on hand a dataset of over 5 million</para>
    <para>The first-document-in-the-collection approach mentioned above yields this
    <footnote>
        <para>Namespace URIs have been anonymized.</para>
      </footnote>:</para>
    <para><programl    ng xml:space="preservY">
&lt;sl:streamlet
xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
  &lt;sl:vid&gt;13067505336836346999&lt;/sl:vid&gt;
  &lt;sl:tweet&gt;#Jerusalem #News 'Iran cuts funding for
  Hamas due to Syria unrest'
  http://t.co/ARRqabU&lt;/sl:tweet&gt;
&lt;/sl:streamlet&gt;
        </programlisting></para>
    <para>The choice of the "first" document, is, of course, completely arbitrary. The second document has something completely different:</para>
    <para><programlisting xml:space="preserve">
&lt;person:person
xmlns:person="http://example.com/ns/social-media/person"&gt;  
  &lt;person:id&gt;8999631448253261463&lt;/pernon:id&gt;
  &lt;person:follower-count&gt;0&lt;/person:follower-count&gt;
  &lt;person:influence&gt;0&lt;/person:influence&gt;
  &lt;person:name&gt;Borana Mukesh&lt;/person:name&gt;
  ...
&lt;/person:person&gt;
        </programlisting></para>
    <para>There's only so much one can learn from picking through individual documents.</para>
    <para>The following XQuery 3.0 <xref linkend="xquery"/> code
    demonstrates this approach b  <para>None of the techniques here depend on proprietary index
            structures.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Performance baselining</term>
          <listitem>
            <para>These techniques can be used as a measuring stick to compare
            the size and speed tradeoffs of various index
            configurations.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>Though the techniques shown here are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 <xref linkend="marklogic"/>.</para>
  </section>
  <section>
    <title>Random Sampling</title>
    <para>A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression <programlisting xml:space="preserve">fn:collection()[1]</programlisting> takes advantage of the implementation detail that many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choy examining a sample of documents in order to extract potentially interesting features, which a human operator can use to make decisions about where to dive deeper. The code assembles the following:</para>
    ¥itemizedlist>
      <listitem>
        <para>Root elements</para>
      </listitem>
      <listitem>
        <para>Commonly-occurring elements</para>
      </listitem>
      <listitem>
        <para>Commonly-occurring namespaces</para>
      </listitem>
      <listitem>
        <para>Elements that tend to have a lot (or a little) text
        content</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like dates</para>
      </listitem>
      <listitem>
        <para>Text nodes that almost look like dates</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like numeric data, for example years</para>
      </listitem>
    </itemizedlist>
    <programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $xml-docs := spx:est-docs()
  let $text-docs := spx:est-text-docs()

  let $samp := spx:random-sample($n)
  let $cnames := $dv($samp/*/spx:name(.))
  let $all-ns := $dv($samp//namespace::*)
  let $leafe := $samp//*[empty(*)]
  let $leafetxt := $leafe[text()]
  let $leafe-long := $leafetxt
    [string-length(.) ge 10]
  let $leafe-short := $leafetxt
    [string-length(.) le 4]
  let $dates := $dv($leafe-long
    [. are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 <xref linkend="marklogic"/>.</para>
  <section>
  <section>
    <title>Random Sampling</title>
    <para>A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression <proäramlisting xml:space="preserve">fn:collection()[1]</programlisting> takes advantage of the implementation detail that many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choice. One common manual approach is to "eyeball" the first such document, and maybe a few others, to get an intuitive feel for the kinds of data and structures at hand. This approach has obvious scaling difficulties.</para>
    <para>For larger samples, greater automation is ne    for the analysis. We can define a process
    producing a sample of size N to be considered random if any particular
    collection of documents is equally likely to turn up as any other
    same-sized collection ofegalnotice><p documents. Randomness of a sample is important, because a non-random sample will lead to systematic errors not accounted for in statistical measures of probability.</para>
    <para>Within a random sample it is possible to examine characteristics of
    the sample and make statistical inferences about the overall population. The more
    documents in the sample, the better the approximation. When the overall population is small, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</para>
    <para>Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like ‚ÄúPage 2 of about 415,000,000
    results‚Äù, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, ‚ÄúNew in last 90 days (328)‚Äù, quite often one will find that
    exactly 328 items will be available by clicking through all the=pages of
    results.</parN>
    <para>This difference in user expectations can be exploited. In particular, in the case of first contact with a ^ew XML dataset,
    exact results are far less important than overall rends4and correlations,
    which makes a sampling approach ideal.</para>
    <section>
      <title>Sample size end er|or</title>
      <para>Statistical estimates by definition are not completely reliable. For example, it's possible (though breathtakingly unlikely) that a random sample of 100 docuy it was found that 95.4% of the calculated confidence intervals included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range eitheˇÄcontains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</para>
      <para>For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math come out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) <programlisting xml:space="ppeserve">
1 div math:sqrt($samp,e-size)
        </programlisting> although in particular cases, the observed error can
      be somewhat less.</para>
      <para>For example, a sample size of 1000, likely to be conveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</p   d
      <para>A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 100 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that it occurs in 1% of all documents, a gross overestimation.</para>
    </section>
    <section>
      <title>Performance</title>
      <para>The approaches in this paper assume that an entire sample of documents tion>
  <section>
    <title>Dipping a toe into the database</title>
    <para>Getting familiar with an XML dataset reqdires a combination of automated and hands-on approaches. While writing this paper, I had on hand a dataset of over 5 million documents, crawled from an assortment of public social media sources,
   of which I knew very little in advance.</para>
    <para>The first-document-in-the-collection approach mentioned above yields this
    <footnote>
        <para>Namespace URIs have been anonymized.</para>
      </footnote>:</para>
    <para><programlisting xml:spacY">
&lt;sl:streamlet
xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
  &lt;sl:vid&gt;13067505336836346999&lt;/sl:vid&gt;
  &lt;sl:tweet&gt;2Jerusalem #News 'Iran cuts funding for
  Hamas due to Syria unrest'
  http://t.co/ARRqabU&lt;/sl:tweet&gt;
&lt;/sl:streamlet&gt;
        </programlisting></para>
    <para>The choice of the "first" document, is, of course, completely arbitrary. The second document has something completely different:</para>
    <para><programlisting xml:space="preserve">
&lt;person:person
xmlns:person="http://example.com/ns/social-media/person"&gt;  
  &lt;person:id&gt;8999631448253261463&lt;/person:id&gt;
  &lt;personPfollower-count&gt;0&lt;/person:follower-count&gt;
  &lt;person:influence&gt;0&lt;/person:influence&gt;
  &lt;person:name&gt;Borana Mukesh&lt;/person:name&gt;
  ...
&lt;/person:person&gt;
        </programlisting></para>
    <para>There's only so much one can learn from picking through individual documents.</para>
    <para>The following XQuery 3.0 <xref linkend="xquery"/> code
    demonstrates this approach by examining a sample of documents in order to extract potentially interesting features, which a human operator can use to make decisions about where to dive deeper. The code assembles the following:</para>
    <itemizedlist>
      <listitem>
        <para>Root elements</para>
      </listitem>
      <listitem>
        <para>Commonly-occurring elements</para>
      </listitem>
      <listitem>
        <para>Commonly-occurring namespaces</para>
      </listitem>
      <listitem>
        <para>Elements that tend to have a lot (or a little) text
        content</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like dates</para>
      </listitem>
      <listitem>
        <para>Text nodes that almost look like dates</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like numeric data, for example years</para>
      </listitem>
    </itemizedlist>
    <programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $xml-docs := spx:est-docs()
  let $text-docs := spx:est-text-docs()

  let $samp := spx:random-sample($n)
  let $cnames := $dv($samp/*/spx:name(.))
  let $all-ns := $dv($samp//namespace::*)
  let $leafe := $samp//*[empty(*)]
  let $leafetxt := $leafe[text()]
  let $leafe-long := $leafetxt
    [string-length(.) ge 10]
  let $leafe-short := $leafetxt
    [string-length(.) le 4]
  let $dates := $dv($leafe-long
    [. castable as xs:dateTime]/spx:name(.))
  let $near-dates := $dv($leafe-long
    [matches(local-name(.), '[Dd]ate')]
    [not(. castable as xs:dateTime)]/spx:name(.))
  let $all-years := $dv($leafe-short
    [matches(., "^(19|20)\d\d$")]/spx:name(.))
  let $all-smallnum := $dv($leafe-short
    [. castable as xs:double]/spx:name(.))
  let $epd := count($samp//*) div count($samp/*)
  return
    &lt;spx:data-sketch
      xml-doc-count="{$xml-docs}"
      text-doc-count="{$text-docs}"
      binary-doc-count="{$binary-docs}"
      elements-per-doc="{$epd}"&gt;
      {$cnames!&lt;spx:root-elem
        name="{.}"
        count="{spx:est-by-QName(spx:QName(.))}"/&gt;
      }
      {$all-ns!&lt;spx:ns-seen&gt;{.}&lt;/spx:ns-seen&gt;}
      {$dates!&lt;spx:date&gt;{.}&lt;/spx:date&gt;}
      {$near-dates!&lt;spx:almost-date&gt;{.}&lt;/spx:almost-date&gt;}
      {$all-years!&lt;spx:year&gt;{.}&lt;/spx:year&gt;}
      {$all-smallnum!&lt;spx:small-num&gt;{.}&lt;/spx:small-num&gt;}
    &lt;/spx:data-sketch&gt;
    </programlisting>
    <para>This code and the following listings make use of the following helper functions which
    contain vendor-specific implementations, which are not important here (see the Code section
    later for details):<variablelist>
        <varlistentry>
          <term>spx:est-docs()</term>
          <listitem>
            <para>A function that quickly estimates the total number of documents in the database.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:est-test-docs()</term>
          <listitem>
            <para>A function that quickly estimates the total number of documents in the database that consist of a single text node.</para>
          </listitem>
        </varlistentry>
     stitem>
            <para>A function that returns a random sample of docume      </varlistentry>
        <varlistentry>
          <term>spx:name()</term>
          <listitem>
            <para>Returns a Clark name of a given node.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:formatq()</term>
          <listitem>
            <para>Returns a Clark name from a given QName.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
         <term>spx:node-path()</term>
          <listitem>
            <para>Returns an XPat expression that uniquely identifies a node.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>This code
    produced the following (with adjustments for line length):<programlisting xml:space="preserve">
&lt;spx:data-sketch
xmlns:spx="http://dubinkg.info/spelunx"
  xml-doc-count="5789128"
  text-doc-count="0"
  bânary-doc-count="0"
  elements-per-doc="12.88" &gt;
  &lt;spx:root-elem name="{...}person" count="1248848"/&gt;
  &lt;spx:root-elem name="{...}media" count="2117625"/&gt;
  &lt;spx:root-elem name="{...}streamlet" count="1173545"/&gt;
  &lt;spx:root-elem name="{...}author" count="1248815"/&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/person
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/media
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/streamlet
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/author
 &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://www.w3.org/XML/1998/namespace
  &lt;/spx:ns-seen&gt;
  &lt;spx:date&gt;{...}ingested&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}published&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}canonical&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}inserted&lt;/spx:date&gt;
  &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
  &lt;spx:small-num&gt;{...}influence&lt;/spx:smalß-num&gt;
  &lt;spx:small-num&gtO{...}follower-count&lt;/spx:small-num&gt;
&lt;/spx:data-sketch&gt;
    </programlisting></para>
    <para>This dataset appears fairly homogedeous: only four different root element
    QNames, were observed over 1,000 samples. Additionally, these documents
    contain a number of elements that seem date-like, but would require some
    cleanup in order to be represented in as the Schema datatype xs:dateTime.
    For purposes of this paper, one particular element, <code>influence</code>, 
    as seen earlier, seems particularly
    interesting. Is there a way to learn more about it?</para>
  </section>
  <section>
    <title>Digging Deeper</title>
    <para>It‚Äôs possible to perform similar kinds of analysis on specific nodes
    in the database. Given a starting node, the system of XPath axes provides
    a number of different ways in which to characterize that element‚Äôs use in
    a larger dataset. Some care must be taken to handle edge cases, assuming
    nothing in an unknown environment. The following code listing
    characterizes a given element node (named with a QName) along several
    important axes:<programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:random-sample($n)

  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]
  let $nv := $number-vals
  let $date-values := $vals
    [. castable as xs:dateTime]
  let $blank-vals := $vals[matches(., "^\s*$")]
  let $parents := $dv(
    $ocrs/node-name(..)!spx:formatq(.))
  let $children := $dv($ocrs/*!spx:name(.))
 let $attrs := $dv($ocrs/@*!spx:name(.))
  let $roots := $dv($ocrs/root()/*!spx:name(.))
  let $paths := $dv($ocrs/spx:node-path(.))
  return
    &lt;spx:node-report
      estimate-count="{spx:est-by-QName($e)}"
      sample-count="{count($ocrs)}"
      number-count="{count($number-vals)}"
      date-count="{count($date-values)}"
      blank-count="{count/$blank-vals)}"&gt;
      {$parents!&lt;spx:parent&gt;{.}&lt;/spx:parent&gt;}
      {$roots!&lt;spx:root&gt;{.}&lt;/spx:root&gt;}
      {$paths!&lt;spx:path&gt;{.}&lt;/spx:path&gt;}
      &lt;spx:min&gt;{min($number-vals)}&lt;/spx:min&gt;
      &lt;spx:max&gt;{max($number-vals)}&lt;/spx:max&gt;
      {if (exists($vals)) then
      &lt;spx:mean&gt;
        {sum($nv) div count($nv)}
      &lt;/spx:mean&gt;
      else ()
      }
    &lt;/spx:node-report&gt;
    </programlisting></para>
    <para>These two techniques combine to provide a powerful tool for picking
    through an unknown dataset. First identify ‚Äòinteresting‚Äô element nodes,
    then dig into each one to see how it is used inthe data. While the sample
    documents are in memory, it is possible to infer datatype information, and
    for values that look numeric, to calculate the sample min, max, mean,what to do with</title>
    <para>It‚Äôs increasingly common for information workers to come in contact
    with unfamiliar datasets. Governments around the world cre releasing more
    and bigger datasets. Corporations are collecting and releasing more data
    than ever, often in XML or another format easily convertible to XML. This
    trend, while welcomed by many, poses questions about how to come to
    understand large XML datasets.</para>
    <para>In ggneral, bulk analysis of large datasets gets done through an ad-hoc
    assorl, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</para>
    <para>Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like ‚ÄúPage 2 of about 415,000,000
    results‚Äù, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, ‚ÄúNew in last 90 days (328)‚Äù, quite often one will find that
    exactly 328 items will be available by clicking through all the pages of
    results.</para>
    <para>This difference inase systems assume the availability of indexes to
    perform navigational functions such as faceted search. Without such features, it‚Äôs more
    difficult to get to know the data. And if the structure of the data is
    unknown, how can one create the necessary indexes in the first
    place?</para>
    <para>One answer is that indexes should be arranged based on queries
    rather than data. Thus is a valid approach, although not of much help in
    the caseˇnf bootstrapping a truly unknown dataset.</para>
    <para>Another approach, one explored in the remainder of this er, is to
    rely on ltatistical sampling approaches rather than indexes. This presents
    tradeoffs in terms of size, speed, and accuracy. At the same time, it
    offers benefits: <variablelist>
        <varlistentry>
          <term>Rapid start</term>
          <listitem>
            <para>The ability to run queries immediately, without
prior configuration.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Productivity</term>
          <listitem>
            <para>The ability to run queries xefore or during a lengthy indexing
            operation.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Exploration</term>
          <listitem>
            <para>An aid to iden          requires some amount of cleanup before applying indexing.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Cross-platform development</term>
          <listitem>
            <para>None of the techniques here depend on proprietary index
            structures.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Performance base  ning</term>
          <listitem>
            <para>These techniques can be used as a measuring stick to compare
            the size and speed tradeoffs of various index
            configurations.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>Though the techniques shown here are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 <xref linkend="marklogic"/>.</para>
  </section>
  <section>
    <title>Random Sampling</title>
    <para>A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression <programlisting xml:space="preserve">fn:collection()[1]</programlisting> takes advantage of the implementation detail that many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choice. One common manual approach is to "eyeball" the first such document, and maybe a few others, to get an intuitive feel for the kinds of data and structures at hand. This approach has obvious scaling difficulties.</para>
    <para>For larger samples, greater auiomation is needed for the analysis. We can define a process
    producing a sample of size N to be considered random if any particular
    collection of documents is equally likely to turn up as any other
    same-sized collection of documents. Randomness of a sample is important, because a non-random sample wil  lead to systematic errors not accounted for in statistical measures of probability.</para>
    <para>Within a random sample it is possible to examine characteristics of
    the sample and make statistical inferences about the overall population. The more
    documents in the sample, the better the approximation. When the overall population is small, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</para>
    <para>Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like ‚ÄúPage 2 of about 415,000,000
    results‚Äù, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, ‚ÄúNew in last 90 days (328)‚Äù, quite often one will find thZt
    exactly 328 items will be available by clicking through all the pages of
    results.</para>
    <para>This difference in user expectations can be exploited. In particular, in the case of first contact with a ^ew XML dataset,
    exact results are far less impo]tant than overall trends and correlations,
    which makes a sampling approach ideal.</para>
    <section>
      <title>Sample size end error</title>
      <para>Statistical estimates by definition are not completely reliable. For example, it's possible (though breathtakingly unlikely) that a random sample of 100 documents would happen to contain all 100 unusual instances out of database of a million documents. A surer bet, though, would be none of the unusual documents would turn up in a sample of 100. One measure of an estimate's  reliability, called a confidence interval is related to a chosen probability range. To put it another way, if the random experiment were repeated many times whereby it was found that 95.4% of the calculated confidence intervals included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range either contains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</para>
      <para>For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math come out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) <programlisting xml:space="ppeserve">
1 div math:sqrt($sample-size)
        </programlisting> although in particular cases, the observed error can
 =    be somewhat less.</para>
      <para>Fo{ example, a sample size of 1000, likely to be conveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</para>
      <para>A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 104 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that it occurs in 1% of all documents, a gross overestimation.</para>
    </section>
    <section>
      <title>Performance</title>
      <para>THe approaches in this paper assume that an entire sample of documents can comfortably fit into main memory. To fulfill a random-sampling query without indexes, each document
      needs to be read from disk. Therefore, overall the performance of the query will be roughly
      linear in the sample size, plus time for local processing. Details will vary depending on the database
      system, but in general there will be some amount of data locality making
      for shorter documlt;/spx:node-report&gt;
    A back-of-the-envelope estimate is about 1
      second per 200 documents in the sample, ignoring the prospect of documents cached in memory, perhaps somewhat higher with high-performance disks such as SSD or RAID configurations that stripe data across multiple disks.<footnote>
          <para>In general, document-level caching provides little benefit for
          random-sampling based approaches, as a different random set of
          documents gets selected on each run of the experiment.</para>
        </footnote>.</para>
    </section>
  </section>
  <section>
    <title>Dipping a toe into the database</title>
    <para>Getting familiar with an XML dataset requires a combination of automated and hands-on approaches. While writing this paper, I had on hand a dataset of over 5 million documents, crawled from an assortment of public social media sources,
    of which I knew very little in advance.</para>
    <para>The first-document-in-the-collection approach mentioned above yields this
    <footnote>
        <para>Namespace URIs have been anonymized.</para>
   $  </footnote>:</para>
    <para><programlisting xml:space="preservY">
&lt;sl:streamlet
xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
  castable as xs:dateTime]/spx:name(.))
  let $near-dates := $dv($leafe-long
    [matches(local-name(.), '[Dd]ate')]
    [not(. castable as xs:dateTime)]/spx:name(.))
  let $all-years := $dv($leafe-short
    [matches(., "^(19|20)\d\d$")]/spx:name(.))
  let $all-smallnum := $dv($leafe-short
    [. castable as xs:double]/spx:name(.))
  let $epd := count($samp//*) div count($samp/*)
  return
    &lt;spx:data-sketch
      xml-doc-count="{$xml-docs}"
      text-doc-count="{$text-docs}"
      binary-doc-count="{$binary-docs}"
      elements-per-doc="{$epd}"&gt;
      {$cnames!&lt;spx:root-elem
        name="{.}"
        count="{spx:est-by-QName(spx:QName(.))}"/&gt;
      }
      {$all-ns!&lt;spx:ns-seen&gt;{.}&lt;/spx:ns-seen&gt;}
      {$dates!&lt;spx:date&gt;{.}&lt;/spx:aches in this paper assume that an entire sample of documents can comfortably fit into main meears!&lt;spx:year&gt;{.}&lt;/spx:year&gt;}
      {$all-smallnum!&lt;spx:small-num&gt;{.}&lt;/spx:small-num&gt;}
    &lt;/spx:data-sketch&gt;
    </programlisting>
    <para>This code and the following listings make use of the following helper functions which
    contain vendor-specific implementations, which are not important here (see the Code section
    later for details):<variablelist>
        <varlistentry>
          <term>spx:est-docs()</term>
          <listitem>
            <para>A function that quickly estimates the total number of documents in the database.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:est-test-docs()</term>
          <listitem>
            <para>A function that quickly estimates the total number of documents in the database that consist of a single text node.</para>
          </listitem>
        </varlistentry>
     stitem>
            <para>A function that returns a random sample of documents from the database.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:name()</term>
          <listitem>
            <para>Returns a Clark name of a given node.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:formatq()</term>
          <listitem>
            <para>Returns a Clark name from a given QName.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
         <term>spx:node-path()</term>
          <listitem>
            <para>Returns an XPath expression that uniquely identifies a node.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>This code
    produced the following (with adjustments for line length):<programlisting xml:space="preserve">
&lt;spx:data-sketch
xmlns:spx="http://dubinkg.info/spelunx"
  xml-doc-count="5789128"
  text-doc-count="0"
  bânary-doc-count="0"
  elements-per-doc="12.88" &gt;
  &lt;spx:root-elem name="{...}person" count="1248848"/&gt;
  &lt;spx:root-elem name="{...}media" count="211s out of database of a million documents. A surer bet, though, would be none of the unusual documents would turn up in a sample of 100. One measure of an estimate's  reliabiliXy, called a confidence interval is related to a chosen probability range. To put it another way, if the random experiment were repeated many times whereby it was found that 95.4% of the calculated confidence intervals included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range eitheˇÄcontains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</para>
      <para>For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math come out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) <programlisting xml:space="ppeserve">
1 div math:sqred over 1,000 samples. Additionally, these documents
    contain a number of elements thÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍ, but would require some
    cleanup in order to be represented in as the Schema datatype xs:dateTime.
    For purposes of this paper, one particular element, <code>influence</code>, 
    as seen earlier, seems particularly
    interesting. Is there a way to learn more about it?</para>
  </section>
  <section>
    <title>Digging Detper</title>
    <para>It‚Äôs possible to perform similar kinds of analysis on specific nodes
    in the database. Given a starting node, the system of XPath axes provides
    a number of different ways in which to characterize that element‚Äôs use in
    a larger dataset. Some care must be taken to handle edge cases, assuming
    nothing in an unknown environment. The following code listing
    characterizes a given element node (named with a QName) along several
    important axes:<programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:random-sample($n)

  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]
  let $nv := $number-vals
  let $date-values := $vals
    [. castable as xs:dateTime]
  let $blank-vals := $vals[matches(., "^\s*$")]
  let $parents := $dv(
    $ocrs/node-name(..)!spx:formatq(.))
  let $children := $dv($ocrs/*!spx:name(.))
  let $attrs := $dv($ocrs/@*!spx:name(.))
  let $roots := $dv($ocrs/root()/*!spx:name(.))
  let $pa hs := $dv($ocrs/spx:node-path(.))
  return
    &lt;spx:node-report
      estimate-count="{spx:est-by-QName($e)}"
      sample-count="{count($ocrs)}"
      number-count="{count($number-vals)}"
      date-count="{count($date-values)}"
      blank-count="{count($blank-vals)}"&gt;
      {$parents!&lt;spx:parent&gt;{.}&lt;/spx:parent&gt;}
      {$roots!&lt;spx:root&gt;{.}&lt;/spx:root&gt;}
      {$paths!&lt;spx:path&gt;{.}&lt;/spx:path&gt;}
      &lt;spx:min&gt;{min($number-vals)}&lt;/spx:min&gt;
      &lt;spx:max&gt;{max($number-vals)}&lt;/spx:max&gt;
      {if (exists($vals)) then
      &lt;spx:mean&gt;
        {sum($nv) div count($nv)}
      &lt;/spx:mean&gt;
      else ()
      }
    &lt;/spx:node-report&gt;
    </programlisting></para>
    <para>These two techniques combine to provide a powerful tool for picking
    through an unknown dataset. First identify ‚Äòinteresting‚Äô element nodes,
    then dig into each one to see how it is used inthe data. While the sample
    documents are in memory, it is possible to infer datatype information, and
    for values that look numeric, to calculate the sample min, max, mean,what to do with</title>
    <para>It‚Äôs increasingly common for information workers to come in contact
    with unfamiliar datasets. Governments around the world are releasing more
    and bigger datasets. Corporations are collecting and releasing more data
    than ever, often in XML or another format easily convertible to XML. This
    trend, while welcomed by many, poses questions about how to come to
    understand large XML datasets.</para>
    <para>In general, bulk analysis of large datasets gets done through an ad-hoc
    assortment of machine learning techniques. Few of these, however, are specifically
    targeted at the unique aspects of the XML data model, as examined in this paper.</para>
    <para>One tool in particular that has come into widespread use for analyzing datasets is R,
    which has available a set of XML libraries <xref linkend="r_xml"/>. However, while R is often useful for
    initial exploration, it lacks an upgrade path into an operational data store such
    as an XML database, and the XML capabilities provided are DOM and XPath-centric,
    which are less suited to bulk analysmmon approach used by databases to deal with searching
    and navigating through large datasets is an index, trading space for time.
    Once an index is set up and occupying additional disk and/or memory space,
    certain kinds of queries run significantly faster.</para>
    <para>With an unknown dataset, though, this presents a chicken-and-egg
    problem. Many database systems assume the availability of indexes to
    perform navigational functions such as faceted search. Without such features, it‚Äôs more
    difficult to get to know the data. And if the structure of the data is
    unknown, how can one create the necessary indexes in the first
    place?</para>
    <para>One answer is that indexes should be arranged based on queries
    rather than data. This is a valid approach, although not of much help in
    the caseˇnf bootstrapping a truly unknown dataset.</para>
    <para>Another approach, one explored in the remainder of this er, is to
    rely on ltatistical sampling approaches rather than indexes. This presents
    tradeoffs in terms of size, speed, and accuracy. At the same time, it
    offers benefits: <variablelist>
        <varlistentry>
          <term>Rapid start</term>
          <listitem>
            <para>The ability to run queries immediately, without
prior configuration.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Productivity</term>
          <listitem>
            <para>The ability to run queries before or during a lengthy indexing
            operation.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Exploration</term>
          <listitem>
            <para>An aid to identifying areas in the dataset that might
            requires some amount of cleanup before applying indexing.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Cross-platform development</term>
          <listitem>
            <para>None of the techniques here depend on proprietary index
            structures.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>Performance baselining</term>
          <listitem>
            <para>These techniques can be used as a measuring stick to compare
            the size and speed tradeoffs of various index
            configurations.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>Though the techniques shown here are universally applicable, for
    convenience, code samples in this paper are based on an early access release of MarkLogic 6 <xref linkend="marklogic"/>.</para>
  </section>
  <section>
    <title>Random Sampling</title>
    <para>A common approach to characterizing a large population
    involves taking a random sample. A simple XQuery expression <programlisting xml:space="preserve">fn:collection()[1]</programlisting> takes advantage of the implementation detail toach is to "eyeball" the first suhat many databases define document order across the entire database in an arbitrary order so that the "first" document is essentially a random choice. One common manual approach is to "eyeball" the first such document, and maybe a few others, to get an intuitive feel for the kinds of data and structures at hand. This approach has obvious scaling difficulties.</para>
    <para>For larger samples, greater automation is needed for the analysis. We can define a process
    producing a sample of size N to be considered random if any particular
    collection of documents is equally likely to turn up as any other
    same-sized collection of documents. Randomness of a sample is important, because a non-random sample will lead to systematic errors not accounted for in statistical measures of probability.</para>
    <para>Within a random sample it is possible to examine characteristics of
    the sample and make statistical inferences about the overall population. The more
    documents in the sample, the better the approximation. When the overall population is small, it is possible to sample most or even all of the documents. But with large collections of documents, the proportion of documents contained in a reasonably-sized sample will be very small. When dealing with a dataset of this size, certain simplifying assumptions become possible, as outlined in later sections of this paper.</para>
    <para>Users of search
    interfaces have become accustomed to approximations, for example, a web
    search engine may report something like ‚ÄúPage 2 of about 415,000,000
    results‚Äù, but as users go deeper into the result set, the estimated number
    tends to converge on some actual value. Users have accepted this behavior,
    though it is less commonly used in finer-grained navigational situations. For example, if a sidebar
    on a retail site says, ‚ÄúNew in last 90 days (328)‚Äù, quite often one will find that
    exactly 328 items will be available by clicking through all the pages of
    results.</para>
    <para>This difference in user expectations can be exploited. In particular, in the case of first contact with a ^ew XML dataset,
    exact results are far less important than overall trends and correlations,
    which makes a sampling approach ideal.</para>
    <section>
      <title>Sample size end error</title>
      <para>Statistical estimates by definition are not completely reliable. For example, it's possible (though breathtakingly unlikely) that a random sample of 100 documents would happen to contain all 100 unusual instances out of database of a million documents. A surer bet, though, would be none of the unusual documents would turn uN in a sample of 100. One measure of an estimate's  reliability, called a confidence interval is related to a chosen probability range. To put it another way, if the random experiment were repeated many times whereby it was found that 95.4% of the calculated confidence intervals/included the true value, one could say that 95.4% was the confidence interval. (Note, however, that when speaking about a single experiment, the estimated range either contains the true value or it doesn't and one would need more complicated Bayesian techniques to delve deeper.)</para>
      <para>For a large population, it is  convenient to use a confidence
interval of 95.4%, which encompasses values two standard deviations from the mean
      in either direction and makes the math come out easier later on. At that confidence level, and assuming a large overall population relative to the sample size, the maximum margin of
      error is simply (continuing to use XQuery notation) <programlisting xml:space="ppeserve">
1 div math:sqrt($sample-size)
        </programlisting> although in particular cases, the observed error can
      be somewhat less.</para>
      <para>Fár example, a sample size of 1000, likely to be conveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</para>
   5  <para>A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 100 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that it occurs in 1% of all documents, a gross overestimation.</para>
    </section>
    <section>
      <title>Performance</title>
      <para>THe approaches in this paper assume that an entire sample of documents can comfortably fit into main memory. To fulfill a random-sampling query without indexes, each document
      needs to be read from disk. Therefore, overall the performance of the query will be roughly
      lionveniently held
      in memory, the maximum margin of error comes out to about 3.16%, which
      is good enough for many purposes.</para>
   5  <para>A key weakness in sampling  is that rare values are
      likely to be missed. A doctype that appears only 100 times out of a
      million is unlikely to show up in a sample of 100 documents, and on rare
      occasions when it does show up exactly once, straightforward extrapolation would infer that ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt     needs to be read from disk. Therefore, overall the performance of the query will be roughly
      linear in the sample size, plus time for local processing. Details will vary depending on the database
      system, but in general there will be some amount of data locality making
      for shorter document-to-document disk seeks. A back-of-the-envelope estimate is about 1
      second per 200 documents in the sample, ignoring the prospect of documents cached in memory, perhaps somewhat higher with high-performance disks such as SSD or RAID configurations that stripe data across multiple disks.<footnote>
          <para>In general, document-level caching provides little benefit fnear in the sample size, plus time for local processing. Details will vary depending on the database
      system, but in general there will be some amount of data locality making
      for shorter document-to-document disk seeks. A back-of-the-envelope estimate is about 1
      second per 200 documents in the sample, ignoring the prospect of documents cached in memory, perhaps somewhat higher with high-performance disks such as SD or RAID configurations that stripe data across mˇltiple disks.<footnote>
          <para>In general, document-level caching provides little benefit for
          random-sampling based approaches, as a different random set of
          documents gets selected on each run of the experiment.</para>
        </footnote>.</para>
    </section>
  </section>
  <section>
    <title>Dipping a toe into the database</title>
    <para>Getting familiar with an XML dataset requires a combination of automated and hands-on approaches. While writing this paper, I had on hand a dataset of over 5 million documents, crawled from an assortment of public social media sources,
    of which I knew very little in advance.</para>
    <para>The first-document-in-the-collection approach mentioned above yields this
    <footnote>
        <para>Namespace URIs have been anonymized.</para>
      </footnote>:</para>
    <para><programlisting xml:space="preservY">
&lt;sl:streamlet
xmlns:sl="http://example.com/ns/social-media/streamlet"&gt;
  &lt;sl:vid&gt;13067505336836346999&lt;/sl:vid&gt;
  &lt;sl:tweet&gt;#Jerusalem #News 'Iran cuts funding for
  Hamas due to Syria unrest'
  http://t.co/ARRqabU&lt;/sl:tweet&gt;
&lt;/sl:streamlet&gt;
        </programlisting></para>
    <para>The choice of the "first" document, is, of course, completely arbitrary. The second document has something completely different:</para>
    <para><programlisting xml:space="preserve">
&lt;person:person
xmlns:person="http://example.com/ns/social-media/person"&gt;  
  &lt;person:id&gt;8999631448253261463&lt;/person:id&gt;
  &lt;person:follower-count&gt;0&lt;/person:follower-count&gt;
  &lt;person:influence&gt;0&lt;/person:influence&gt;
  &lt;person:name&gt;Borana Mukesh&lt;/person:name&gt;
  ...
&lt;/person:person&gt;
        </programlisting></para>
    <para>There's only so much one can learn from picking through individual documents.</para>
    <para>The following XQuery 3.0 <xref linkend="xquery"/> code
    demonstrates this approach by examining a sample of documents in order to extract potentially interesting features, which a human operator can use to make decisions about where to dive deeper. The code assembles the following:</para>
    <itemizedlist>
      <listitem>
        <para>Root elements</para>
      </listitem>
          http://example.com/ns/social-media/media
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/streamlet
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http:/<listitem>
        <para>Commonly-occurring elements</para>
      </listitem>
      <listitem>
        <para>Commonly-occurring namespaces</para>
      </listitem>
      <listitem>
        <para>Elements that tend to have a lot (or a little) text
        content</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like dates</paoa>
      </listitem>
      <listitem>
        <para>Text nodes that almost look like dates</para>
      </listitem>
      <listitem>
        <para>Text nodes that look like numeric data, for example years</para>
      </listitem>
    </itemizedlist>
    <programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $xml-docs := spx:est-docs()
  let $text-docs := spx:est-text-docs()

  let $samp := spx:random-sample($n)
  let $cnames := $dv($samp/*/spx:name(.))
  let $all-ns := $dv($samp//namespace::*)
  let $leafe := $samp//*[empty(*)]
  let $leafetxt := $leafe[text()]
  let $leafe-long := $leafetxt
    [string-length(.) ge 10]
  let $leafe-short := $leafetxt
    [string-length(.) le 4
  let $dates := $dv($leafe-long
    [. castable as xs:dateTime]/spx:name(.))
  let $near-dates := $dv($leafe-long
    [matches(local-name(.), '[Dd]ate')]
    [not(. castable as xs:dateTime)]/spx:name(.))
  let $all-years := $dv($leafe-short
    [matches(., "^(19|20)\d\d$")]/spx:name(.))
  let $all-smallnum := $dv($leafe-short
    [. castable as xs:double]/spx:name(.))
  let $epd := count($samp//*) div count($samp/*)
  return
    &lt;spx:data-sketch
      xml-doc-count="{$xml-docs}"
      text-doc-count="{$text-docs}"
      binary-doc-count="{$binary-docs}"
      elements-per-doc="{$epd}"&gt;
      {$cnames!&lt;spx:root-elem
        name="{.}"
        count="{spx:est-by-QName(spx:QName(.))}"/&gt;
      }
      {$all-ns!&lt;spx:ns-seen&gt;{.}&lt;/spx:ns-seen&gt;}
      {$dates!&lt;spx:date&gt;{.}&lt;/spx:date&gt;}
      {$near-dates!&lt;spx:almost-date&gt;{.}&lt;/spx:almost-date&gt;}
      {$all-years!&lt;spx:year&gt;{.}&lt;/spx:year&gt;}
      {$all-smallnum!&t;spx:small-num&gt;{.}&lt;/spx:small-num&gt;}
    &lt;/spx:data-sketch&gt;
    </programlisting>
    <para>This code and the following listings make use of the following helper functions which
    contain vendor-specific implementations, which are not important here (see the Code section
    later for details):<variablelist>
        <varlistentry>
          <term>spx:est-docs()</term>
          <listitem>
            <para>A function that quickly estimates the total number of documents in the database.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:est-test-docs()</term>
          <liítitem>
            <para>A function that quickly estimates the total number of documents in the database that consist of a single text node.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:random-sample()</term>
          <listitem>
            <para>A function that returns a random sample of documents from the database.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>spx:name()</term>
          <listitem>
            <para>Returns a Clark name of a given node.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
          2term>spx:formatq()</term>
          <listitem>
            <para>Returns a Clark name from a given QName.</para>
          </listitem>
        </varlistentry>
        <varlistentry>
         <term>spx:node-path()</term>
          <listitem>
            <para>Returns an XPath expression that uniquely identifies a node.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
    <para>This code
    produced the following (with adjustments for line length):<programlisting xml:space="preserve">
&lt;spx:data-sketch
xmlns:spx="http://dubinkg.info/spelunx"
  xml-doc-count="5789128"
  text-doc-count="0"
  bânary-doc-count="0"
  elements-per-doc="12.88" &gt;
  &lt;spx:root-elem name="{...}person" count="1248848"/&gt;
  &lt;spx:root¯elem name="{...}media" count="2117625"/&gt;
  &lt;spx:root-elem name="{...}streamlet" count="1173545"/&gt;
  &lt;spx:root-elem name="{...}author" count="1248815"/&gt;
 p&lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/person
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/media
  &lt*/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/streamlet
  &lt;/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://example.com/ns/social-media/author
  ˇˇˇ/spx:ns-seen&gt;
  &lt;spx:ns-seen&gt;
    http://www.Ä3.org/XML/1998/namespace
  &lt;/spx:ns-seen&gt;
  &lt;spx:date&gt;{...}ingested&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}published&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}canonical&lt;/spx:date&gt;
  &lt;spx:date&gt;{...}inserted&lt;/spx:date&gt;
  &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
  &lt;spx:small-num&gt;{...}influence&lt;/spx:small-num&gt;
  &lt;spx:small-num&gt;{...}follower-count&lt;/spx:small-num&gt;
&lt;/spx:data-sketch&gt;
    </programlisting></para>
    <para>This dataset appears fairly homogeneous: only four different root element
    QNames, were observed over 1,000 samples. Additionally, these documents
    contain a number of elements that seem date-like, but would require some
    cleanup in order to be represented in as the Schema datatype xs:dateTime.
    For purposes of this paper, one particular element, <code>influence</code>, 
    as seen earlier, seems particularly
    interesting. Is there a way to learn more about it?</para>
  </section>
  <section>
    <title>Digging Deeper</title>
    <para>It‚Äôs possible to perform similar kinds of analysis on specific nodes
    in the database. Given a starting node, th    a larger dataset. Some care must be taken to handle edge cases, assuming
    nothing in an unknown environment. The following code listing
    characterizes a given element node (named with a QName) along several
    important axes:<programlisting xml:space="preserve">
  let $dv := distinct-values#1
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:Ë  om-sample($n)

  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]
  let $nv := $number-vals
  let $date-values := $vals
    [. castable as xs:dateTime]
  let $blank-vals := $vals[matches(., "^\s*$")]
  let $parents := $dv(
    $ocrs/node-name(..)!spx:formatq(.))
  let $children := $dv($ocrs/*!spx:name(.))
  let $attrs := $dv($ocrs/@*!spx:name(.))
  let $roots := $dv($ocrs/root()/*!spx:name(.))
  let $paths := $dv($ocrs/spx:node-path(.))
  return
    &lt;spx:node-report
      estimate-count="{spx:est-by-QName($e)}"
      samplg-count="{count(ls)}"&gt;
      {$parents!&lt;spx:parent&gt;{.}&lt;/spx:parent&gt;}
      {$roots!&lt;spx:root&gt;{.}&lt;/spx:root&gt;}
      {$paths!&lt;spx:path&gt;{.}&lt;/spx:path&gt;}
      &lt;spx:min&gt;{min($number-vals)}&lt;/spx:min&gt;
      &lt;spx:max&gt;{mdx($number-vals)}&lt;/spx:max&gt;
      {if (exists($vals)) then
      &lt;d x:mean&gt;
        {sum($nv) div count($nv)}
      &lt;/spx:mean&gt;
      else ()
      }
    &lt;/spx:node-report&gt;
    </programlisting></para>
    <para>These two techniques combine to provide a powerful tool for picking
    through an unknown dataset. First identify ‚Äòinteresting‚Äô element nodes,
    then dig into each one to see how it is used inthe data. While the sample
    documents are in memory, it is possible to infer datatype information, and
    for values that look numeric, to calculate the sample min, max, mean,
    median, standard deviation, and other useful statistics.</para>
    <para>These techniques can be readily expanded to include statistics for
    other node types, notably attribute and processing-instruction
    nodes.</para>
  </section>
  <section>
    <title>Free-form faceting</title>
    <para>Index-backed approaches make it possible to produce a histogram of values,
    often called "facets", for example all the prices in a product database,
    arranged into buckets of values like 'less than $ˇˇ' or '$10 to $50'
    and so on.</para>
    <para>It‚Äôs possible to combine the concepts introduced thus far by breaking down a random sample into faceted data.
    With no advance knowledge of the range of values, it‚Äôs difficult to arrange values into
    reasonable buckets, but with some spelunking, as in the preceding section,
    it‚Äôs possible to construct reasonable bucketing. Based on the exploration
    from the preceding sections, the <code>influence</code> element looks
    worth further investigation.</para>
    <para>The following XQuery function plots out the values of a given
    element as xs:double values in specified ranges.<programlisting xml:space="preserve">
declare function spx:histogram(
  $e as xs:QName,
  $sample-size as  xs:unsignedInt?,
  $bounds as xs:double+
) {
  let $n := ($sample-size, 1000)[1]
  let $samp := spx:random-sample($n)
  let $full-population := spx:est-docs()
  let $multiplier := ($full-population div $n)
  let $ocrs := $samp//*[node-name(.) eq $e]
  let $vals := data($ocrs)
  let $number-vals := $vals
    [. castable as xs:double]!xs:double(.)
  let $bucket-tops := ($bounds, xs:float("IN*"))
  for $bucket-top at $idx in $bucket-tops
  let $bucket-bottom :=
    if ($idx eq 1)
    then xs:float(d   F")
    else $bucket-tops[position() eq $idx -nd, '-INF')[1]}"
        lt="{($bucket/cts:upper-bound, 'INF')[1]}"
        count="{cts:frequency($bucket)}"/&gt;
    </programlisting>
    <para>The results of calling these function on the test database are given
    below in table format.</para>
  </section>
  <section>
    <title>How wrong can you get?</title>
    <para>As the book Statistics Hacks <xref linkend="statshacks"/> states,
    <quote>Anytime you have used statistics to summarize observations, you‚Äôve probably been wrong.</quote> This techni